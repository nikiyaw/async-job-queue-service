# Asynchronous Job Queue Service

## Overview
The Asynchronous Job Queue Service is a backend system for handling long-running or resource-intensive tasks outside the request/response cycle.
It allows users to submit jobs via an API or dashboard, process them asynchronously with Celery workers, and monitor their progress in real time.

This project demonstrates key backend concepts:
- Database persistence with PostgreSQL & SQLAlchemy
- Asynchronous task processing using Celery & Redis
- Real-time monitoring through a clean dashboard UI
- Containerized, production-ready setup with Docker Compose and tests

### Features
- ğŸ“¦ Submit jobs through a RESTful API or dashboard form
- âš¡ Process tasks asynchronously with Celery and Redis
- ğŸ—„ï¸ Persistent storage in PostgreSQL with SQLAlchemy ORM
- ğŸ“Š Real-time job status dashboard (Tailwind + Vanilla JS)
- ğŸ³ Containerized with Docker Compose (API, Worker, DB, Redis)
- ğŸ§ª Pytest-based test suite with SQLite in-memory support
- ğŸ“ Structured logging with optional JSON logs

### Tech Stack
- API framework: **FastAPI**
- Distributed task queue: **Celery**
- Message broker and result backend: **Redis**
- Relational database: **PostgreSQL**
- ORM: **SQLAlchemy**
- Frontend dashboard: **TailwindCSS + JS**
- Local orchestration: **Docker Compose**
- Testing: **Pytest**

### Project Structure
project-root/
â”œâ”€ src/
â”‚  â”œâ”€ api/
â”‚  â”‚  â”œâ”€ core/
â”‚  â”‚  â”‚  â”œâ”€ settings.py
â”‚  â”‚  â”‚  â”œâ”€ database.py
â”‚  â”‚  â”‚  â”œâ”€ celery_app.py
â”‚  â”‚  â”‚  â””â”€ logging_config.py
â”‚  â”‚  â”œâ”€ models/
â”‚  â”‚  â”‚  â”œâ”€ job.py
â”‚  â”‚  â”‚  â””â”€ sql_models/
â”‚  â”‚  â”‚     â””â”€ job.py
â”‚  â”‚  â”œâ”€ routers/
â”‚  â”‚  â”‚  â””â”€ jobs.py
â”‚  â”‚  â”œâ”€ static/
â”‚  â”‚  â”‚  â””â”€ js/app.js
â”‚  â”‚  â”œâ”€ templates/
â”‚  â”‚  â”‚  â””â”€ index.html
â”‚  â”‚  â””â”€ main.py
â”‚  â””â”€ worker/
â”‚     â”œâ”€ celery_worker.py
â”‚     â””â”€ db_utils.py
â”œâ”€ tests/
â”‚  â”œâ”€ conftest.py
â”‚  â””â”€ test_api.py
â”œâ”€ docker-compose.yml
â”œâ”€ Dockerfile
â”œâ”€ requirements.txt
â””â”€ README.md

### Getting Started
1. Clone the repository
   ```
   git clone https://github.com/your-username/async-job-queue-service.git
   cd async-job-queue-service
   ```
   
2. Configuration
   The project uses pydantic-settings for configuration. Default values are defined in src/api/core/settings.py.
   By default, the app will connect to:
   - postgresql://user:password@db:5432/jobs_db
   - redis://redis:6379/0
   You can override any of these by creating a .env file in the project root:
   ```
   DATABASE_URL=postgresql://your_user:your_pass@localhost:5432/your_db
   REDIS_URL=redis://localhost:6379/0
   CELERY_BROKER_URL=redis://localhost:6379/1
   CELERY_RESULT_BACKEND=redis://localhost:6379/1
   ```

3. Run with Docker Compose
   ```
   docker-compose up --build
   ```
   This starts:
   - FastAPI app (http://localhost:8000)
   - Celery worker
   - PostgreSQL database
   - Redis broker
   
4. Access the dashboard
   Open your browser at:
   (http://localhost:8000/)

5. API Documentation
   Interactive Swagger UI:
   (http://localhost:8000/docs)
   
### Development
1. Run test
   ```
   pytest
   ```
